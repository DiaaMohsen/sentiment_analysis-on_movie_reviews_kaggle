{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils import blabla, print_report, predict_pipeline\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "\n",
    "PICKLED_DIR = \"pickled_files\"\n",
    "DATA_DIR = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_df = shuffle(pd.read_csv(\"%s/train.tsv\" %DATA_DIR, error_bad_lines=False , sep='\\t'))[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment\n",
      "0     7072\n",
      "1    27273\n",
      "2    79582\n",
      "3    32927\n",
      "4     9206\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print training_df.groupby(['Sentiment']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize Sentiment column values from\n",
    "#### [Negative:0, Somewhat Negative:1, Neutral:2, Somewhat Positive:3, Positive:4]  to [Negative:-1, Neutral:0, Positive:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3class_sent\n",
      "-1    34345\n",
      " 0    79582\n",
      " 1    42133\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "training_df['3class_sent'] = training_df['Sentiment'].map({0:-1, 1:-1, 2:0, 3:1, 4:1})\n",
    "print training_df.groupby(['3class_sent']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the 3 Classifiers and pickle them and get their X_test/Y_test\n",
    "### *Read blabla for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diaa/anaconda2/lib/python2.7/site-packages/sklearn/model_selection/_split.py:2010: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.6 s, sys: 198 ms, total: 20.8 s\n",
      "Wall time: 20.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_test1, Y_test1 = blabla(training_df, c_no=1, on_=1)#, label_col='3class_sent')\n",
    "X_test2, Y_test2 = blabla(training_df[training_df['Sentiment']<2], c_no=2, on_=0)#, label_col=\"Sentiment\")\n",
    "X_test3, Y_test3 = blabla(training_df[training_df['Sentiment']>2], c_no=3, on_=0)#, label_col=\"Sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load 3 classifiers and thier vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v1 = pickle.load(open(\"%s/vectorizer_1.pickle\" %PICKLED_DIR, \"rb\"))\n",
    "model1 = pickle.load(open(\"%s/model_1.pickle\" %PICKLED_DIR, \"rb\"))\n",
    "\n",
    "v2 = pickle.load(open(\"%s/vectorizer_2.pickle\" %PICKLED_DIR, \"rb\"))\n",
    "model2 = pickle.load(open(\"%s/model_2.pickle\" %PICKLED_DIR, \"rb\"))\n",
    "\n",
    "v3 = pickle.load(open(\"%s/vectorizer_3.pickle\" %PICKLED_DIR, \"rb\"))\n",
    "model3 = pickle.load(open(\"%s/model_3.pickle\" %PICKLED_DIR, \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report results of Classifier-1 on all data on '3c_sentiment' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   NEGATIVE       0.73      0.66      0.69     10389\n",
      "    NEUTRAL       0.75      0.81      0.78     23801\n",
      "   POSITIVE       0.78      0.71      0.74     12628\n",
      "\n",
      "avg / total       0.75      0.75      0.75     46818\n",
      "\n",
      "accuracy :  0.750929129822\n"
     ]
    }
   ],
   "source": [
    "print_report(Y_test1[:, 1], model1.predict(v1.transform(X_test1)), ['NEGATIVE', 'NEUTRAL', 'POSITIVE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report results of Classifier-2 on data['Sentiment']<2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "         NEGATIVE       0.44      0.55      0.49      2092\n",
      "SOMEWHAT NEGATIVE       0.88      0.83      0.85      8212\n",
      "\n",
      "      avg / total       0.79      0.77      0.78     10304\n",
      "\n",
      "accuracy :  0.768827639752\n"
     ]
    }
   ],
   "source": [
    "print_report(Y_test2[:, 0], model2.predict(v2.transform(X_test2)), ['NEGATIVE', 'SOMEWHAT NEGATIVE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report results of Classifier-3 on data['Sentiment']>2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "SOMEWHAT POSITIVE       0.81      0.88      0.85      9136\n",
      "         POSITIVE       0.60      0.48      0.53      3504\n",
      "\n",
      "      avg / total       0.76      0.77      0.76     12640\n",
      "\n",
      "accuracy :  0.76835443038\n"
     ]
    }
   ],
   "source": [
    "print_report(model3.predict(v3.transform(X_test3)), Y_test3[:, 0], ['SOMEWHAT POSITIVE', 'POSITIVE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Pipeline to predict coming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_sent = predict_pipeline(X_test1, model1, v1, model2, v2, model3, v3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report results on testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "         NEGATIVE       0.77      0.56      0.65      2989\n",
      "SOMEWHAT NEGATIVE       0.48      0.61      0.54      6432\n",
      "          NEUTRAL       0.81      0.75      0.78     25869\n",
      "SOMEWHAT POSITIVE       0.52      0.66      0.58      7742\n",
      "         POSITIVE       0.80      0.59      0.68      3786\n",
      "\n",
      "      avg / total       0.71      0.69      0.70     46818\n",
      "\n",
      "accuracy :  0.688602674185\n"
     ]
    }
   ],
   "source": [
    "print_report(pred_sent, Y_test1[:, 0], ['NEGATIVE', 'SOMEWHAT NEGATIVE', 'NEUTRAL', 'SOMEWHAT POSITIVE', 'POSITIVE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"%s/test.tsv\" %DATA_DIR, error_bad_lines=False , sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test_res = predict_pipeline(test_df['Phrase'].tolist(), model1, v1, model2, v2, model3, v3)\n",
    "\n",
    "# tdf = pd.DataFrame({'PhraseId': test_df['PhraseId'].tolist(), 'Sentiment': test_res})\n",
    "# tdf.to_csv(\"3classes_sent_SVM.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
