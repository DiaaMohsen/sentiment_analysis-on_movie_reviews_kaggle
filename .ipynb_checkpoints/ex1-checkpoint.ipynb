{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# from utils import *#blabla, print_report, predict, prepare_data_alittle\n",
    "# import re\n",
    "# from nltk.corpus import stopwords\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.metrics import precision_recall_fscore_support\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.utils import shuffle\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from submit import submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = shuffle(pd.read_csv(\"train.tsv\", error_bad_lines=False , sep='\\t'))\n",
    "training_df['3class_sent'] = training_df['Sentiment'].map({0:-1, 1:-1, 2:0, 3:1, 4:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print training_df.groupby(['Sentiment']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_df['3class_sent'] = training_df['Sentiment'].map({0:-1, 1:-1, 2:0, 3:1, 4:1})\n",
    "# print training_df.groupby(['3class_sent']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from submit import submit\n",
    "\n",
    "def prepare_data_alittle(text):\n",
    "    return [re.sub('_|#|:|\\n|,|;|«|»|!|-|%|$|@|<|>|\\.|\\...',' ',t) for t in text]\n",
    "\n",
    "def prepare_split_data(training_df, label_col='Sentiment'):\n",
    "\n",
    "    training_temp, testing_temp = train_test_split(training_df, train_size=.7)    \n",
    "\n",
    "    X_train = prepare_data_alittle(training_temp['Phrase'].tolist())\n",
    "    X_test = prepare_data_alittle(testing_temp['Phrase'].tolist())\n",
    "    Y_train = training_temp[label_col].tolist()\n",
    "    Y_test = testing_temp[label_col].tolist()\n",
    "    \n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "def train(X_train, Y_train, which_v, model):\n",
    "    \n",
    "    vectorizer = CountVectorizer(ngram_range=(1,3), stop_words = list(set(stopwords.words(\"english\")) ))\n",
    "    if which_v != 1:\n",
    "        vectorizer = TfidfVectorizer(ngram_range=(1,3), stop_words = list(set(stopwords.words(\"english\")) ))\n",
    "        \n",
    "    tr_features = vectorizer.fit_transform(X_train)\n",
    "    print \"1\", vectorizer.get_params()\n",
    "\n",
    "    print tr_features.shape[0]\n",
    "    \n",
    "    model = model.fit(tr_features, Y_train)\n",
    "    \n",
    "    return model, vectorizer\n",
    "\n",
    "def predict(model, vectorizer, X_test):\n",
    "    vectorizer = CountVectorizer(ngram_range=(1,3), stop_words = list(set(stopwords.words(\"english\")) ))\n",
    "    print \"2\", vectorizer.get_params()\n",
    "    te_features = vectorizer.transform(X_test)\n",
    "    \n",
    "    return model.predict(te_features)\n",
    "\n",
    "def blabla(training_df, label_col='Sentiment', which_v=1, model=LinearSVC(C=.1, class_weight=\"balanced\")):\n",
    "    X_train, X_test, Y_train, Y_test = prepare_split_data(training_df, label_col)\n",
    "    model, vectorizer = train(X_train, Y_train, which_v=which_v, model=model)\n",
    "    predicted_results = predict(model, vectorizer, X_test)\n",
    "    \n",
    "#    return model, vectorizer, predicted_results\n",
    "    \n",
    "    return (X_train, X_test, Y_train, Y_test, model, vectorizer, predicted_results)\n",
    "\n",
    "def print_report(Y_test, predicted_results, target_names):\n",
    "    report = classification_report(Y_test, predicted_results.tolist(), target_names=target_names)\n",
    "    print(report)\n",
    "\n",
    "\n",
    "    print 'accuracy : ' , accuracy_score(Y_test, predicted_results.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109242\n",
      "1 {'binary': False, 'lowercase': True, 'stop_words': [u'all', u'just', u\"don't\", u'being', u'over', u'both', u'through', u'yourselves', u'its', u'before', u'o', u'don', u'hadn', u'herself', u'll', u'had', u'should', u'to', u'only', u'won', u'under', u'ours', u'has', u\"should've\", u\"haven't\", u'do', u'them', u'his', u'very', u\"you've\", u'they', u'not', u'during', u'now', u'him', u'nor', u\"wasn't\", u'd', u'did', u'didn', u'this', u'she', u'each', u'further', u\"won't\", u'where', u\"mustn't\", u\"isn't\", u'few', u'because', u\"you'd\", u'doing', u'some', u'hasn', u\"hasn't\", u'are', u'our', u'ourselves', u'out', u'what', u'for', u\"needn't\", u'below', u're', u'does', u\"shouldn't\", u'above', u'between', u'mustn', u't', u'be', u'we', u'who', u\"mightn't\", u\"doesn't\", u'were', u'here', u'shouldn', u'hers', u\"aren't\", u'by', u'on', u'about', u'couldn', u'of', u\"wouldn't\", u'against', u's', u'isn', u'or', u'own', u'into', u'yourself', u'down', u\"hadn't\", u'mightn', u\"couldn't\", u'wasn', u'your', u\"you're\", u'from', u'her', u'their', u'aren', u\"it's\", u'there', u'been', u'whom', u'too', u'wouldn', u'themselves', u'weren', u'was', u'until', u'more', u'himself', u'that', u\"didn't\", u'but', u\"that'll\", u'with', u'than', u'those', u'he', u'me', u'myself', u'ma', u\"weren't\", u'these', u'up', u'will', u'while', u'ain', u'can', u'theirs', u'my', u'and', u've', u'then', u'is', u'am', u'it', u'doesn', u'an', u'as', u'itself', u'at', u'have', u'in', u'any', u'if', u'again', u'no', u'when', u'same', u'how', u'other', u'which', u'you', u\"shan't\", u'shan', u'needn', u'haven', u'after', u'most', u'such', u'why', u'a', u'off', u'i', u'm', u'yours', u\"you'll\", u'so', u'y', u\"she's\", u'the', u'having', u'once'], 'decode_error': u'strict', 'vocabulary': None, 'tokenizer': None, 'encoding': u'utf-8', 'dtype': <type 'numpy.int64'>, 'analyzer': u'word', 'ngram_range': (1, 3), 'max_df': 1.0, 'min_df': 1, 'max_features': None, 'input': u'content', 'strip_accents': None, 'token_pattern': u'(?u)\\\\b\\\\w\\\\w+\\\\b', 'preprocessor': None}\n",
      "2 {'binary': False, 'lowercase': True, 'stop_words': [u'all', u'just', u\"don't\", u'being', u'over', u'both', u'through', u'yourselves', u'its', u'before', u'o', u'don', u'hadn', u'herself', u'll', u'had', u'should', u'to', u'only', u'won', u'under', u'ours', u'has', u\"should've\", u\"haven't\", u'do', u'them', u'his', u'very', u\"you've\", u'they', u'not', u'during', u'now', u'him', u'nor', u\"wasn't\", u'd', u'did', u'didn', u'this', u'she', u'each', u'further', u\"won't\", u'where', u\"mustn't\", u\"isn't\", u'few', u'because', u\"you'd\", u'doing', u'some', u'hasn', u\"hasn't\", u'are', u'our', u'ourselves', u'out', u'what', u'for', u\"needn't\", u'below', u're', u'does', u\"shouldn't\", u'above', u'between', u'mustn', u't', u'be', u'we', u'who', u\"mightn't\", u\"doesn't\", u'were', u'here', u'shouldn', u'hers', u\"aren't\", u'by', u'on', u'about', u'couldn', u'of', u\"wouldn't\", u'against', u's', u'isn', u'or', u'own', u'into', u'yourself', u'down', u\"hadn't\", u'mightn', u\"couldn't\", u'wasn', u'your', u\"you're\", u'from', u'her', u'their', u'aren', u\"it's\", u'there', u'been', u'whom', u'too', u'wouldn', u'themselves', u'weren', u'was', u'until', u'more', u'himself', u'that', u\"didn't\", u'but', u\"that'll\", u'with', u'than', u'those', u'he', u'me', u'myself', u'ma', u\"weren't\", u'these', u'up', u'will', u'while', u'ain', u'can', u'theirs', u'my', u'and', u've', u'then', u'is', u'am', u'it', u'doesn', u'an', u'as', u'itself', u'at', u'have', u'in', u'any', u'if', u'again', u'no', u'when', u'same', u'how', u'other', u'which', u'you', u\"shan't\", u'shan', u'needn', u'haven', u'after', u'most', u'such', u'why', u'a', u'off', u'i', u'm', u'yours', u\"you'll\", u'so', u'y', u\"she's\", u'the', u'having', u'once'], 'decode_error': u'strict', 'vocabulary': None, 'tokenizer': None, 'encoding': u'utf-8', 'dtype': <type 'numpy.int64'>, 'analyzer': u'word', 'ngram_range': (1, 3), 'max_df': 1.0, 'min_df': 1, 'max_features': None, 'input': u'content', 'strip_accents': None, 'token_pattern': u'(?u)\\\\b\\\\w\\\\w+\\\\b', 'preprocessor': None}\n"
     ]
    },
    {
     "ename": "NotFittedError",
     "evalue": "CountVectorizer - Vocabulary wasn't fitted.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-78c864396da8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclassifier_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblabla\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"3class_sent\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# print_report(classifier_1[3], classifier_1[6], ['Negative', 'Neutral', 'Positive'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# [\"negative\", \"somewhat negative\", \"neutral\", \"somewhat positive\", \"positive\"]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-674bd37a2b42>\u001b[0m in \u001b[0;36mblabla\u001b[0;34m(training_df, label_col, which_v, model)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_split_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhich_v\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwhich_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mpredicted_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;31m#    return model, vectorizer, predicted_results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-674bd37a2b42>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(model, vectorizer, X_test)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mte_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mte_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/diaa/anaconda2/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m    918\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/diaa/anaconda2/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m_check_vocabulary\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0;34m\"\"\"Check if vocabulary is empty or missing (not fit-ed)\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"%(name)s - Vocabulary wasn't fitted.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'vocabulary_'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/diaa/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall_or_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFittedError\u001b[0m: CountVectorizer - Vocabulary wasn't fitted."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "classifier_1 = blabla(training_df, label_col=\"3class_sent\")\n",
    "# print_report(classifier_1[3], classifier_1[6], ['Negative', 'Neutral', 'Positive'])\n",
    "# [\"negative\", \"somewhat negative\", \"neutral\", \"somewhat positive\", \"positive\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for negative\n",
    "classifier_2 = blabla(training_df[training_df['Sentiment']<2], 'Sentiment')\n",
    "print_report(classifier_2[3], classifier_2[6], ['Negative', 'Somewhat Negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CountVectorizer' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-68f13380ef0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclassifier_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'CountVectorizer' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "classifier_1[5].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blabla2(training_df, label_col='Sentiment', which_v=1, model=LinearSVC()):\n",
    "    X_train, X_test, Y_train, Y_test = prepare_split_data(training_df, label_col)\n",
    "    model, vectorizer = train(X_train, Y_train, which_v=which_v, model=model)\n",
    "    predicted_results = predict(model, vectorizer, X_test)\n",
    "    \n",
    "    return (model, vectorizer, predicted_results)\n",
    "\n",
    "    te_f = vectorizer.transform(X_test)\n",
    "    model.predict(te_f)\n",
    "    \n",
    "    \n",
    "# blabla2(training_df)\n",
    "#     predicted_results = predict(model, vectorizer, X_test)\n",
    "#     return (X_train, X_test, Y_train, Y_test, model, vectorizer, predicted_results)\n",
    "#                0        1         2      3       4       5            6  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vec = CountVectorizer(ngram_range=(1,3), stop_words = list(set(stopwords.words(\"english\")) ))\n",
    "# rv = vec.fit_transform(classifier_1[0])\n",
    "\n",
    "# mm = LinearSVC()\n",
    "# mm = mm.fit(rv, classifier_1[2])\n",
    "\n",
    "# te_features = vec.transform(classifier_1[1])\n",
    "# mm.predict(te_features)\n",
    "# classifier_1[1][:5]\n",
    "# predict(classifier_1[4], classifier_1[5], classifier_1[1][:6])\n",
    "\n",
    "# predict\n",
    "# print (classifier_1[5])\n",
    "te_f = classifier_1[5].transform(classifier_1[1][:6])\n",
    "# classifier_2[4].predict(te_f)\n",
    "\n",
    "# a = blabla2(training_df[training_df['Sentiment']<2])\n",
    "# predict(a[0], a[1], classifier_2[1][:6])\n",
    "\n",
    "# predict(classifier_2[4], classifier_2[5], classifier_2[1][:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_predicted_res = classifier_1[-1]\n",
    "for i, t in enumerate(training_df.Phrase.tolist()):\n",
    "    if classifier_1[-1][i] == -1: #NEGATIVE\n",
    "        global_predicted_res[i] = predict(classifier_2[4], classifier_2[5], [t])\n",
    "    elif classifier_1[-1][i] == 1:\n",
    "        global_predicted_res[i] = predict(classifier_3[4], classifier_3[5], [t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for positive\n",
    "classifier_3 = blabla(training_df[training_df['Sentiment']>2], 'Sentiment')\n",
    "print_report(classifier_3[3], classifier_3[6], ['Somewhat Positive', 'Positive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # training_df[training_df['3class_sent']==1].head()\n",
    "# for i, r in training_df.iterrows():\n",
    "# #     classifier_2.\n",
    "#     print r.Phrase\n",
    "#     print predict(classifier_2[4], classifier_2[5], [r.Phrase])\n",
    "#     break\n",
    "#     # print training_df.iloc(0)\n",
    "# # print training_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# submit(model, countVec, \"LinearSVM_CVec_without_downsampling_balanced_p\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
