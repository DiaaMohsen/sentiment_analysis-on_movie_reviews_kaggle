{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils import blabla, print_report\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "\n",
    "PICKLED_DIR = \"pickled_files\"\n",
    "DATA_DIR = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_df = shuffle(pd.read_csv(\"%s/train.tsv\" %DATA_DIR, error_bad_lines=False , sep='\\t'))[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment\n",
      "0     7072\n",
      "1    27273\n",
      "2    79582\n",
      "3    32927\n",
      "4     9206\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print training_df.groupby(['Sentiment']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3class_sent\n",
      "-1    34345\n",
      " 0    79582\n",
      " 1    42133\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "training_df['3class_sent'] = training_df['Sentiment'].map({0:-1, 1:-1, 2:0, 3:1, 4:1})\n",
    "print training_df.groupby(['3class_sent']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SGDClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-15c3a40d169b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu'X_test1, Y_test1 = blabla(training_df, 1, 1, label_col=\\'3class_sent\\', model=SGDClassifier(loss=\"hinge\", penalty=\"l2\"))\\nX_test2, Y_test2 = blabla(training_df[training_df[\\'Sentiment\\']<2], 2, 0, label_col=\"Sentiment\", model=SGDClassifier(loss=\"hinge\", penalty=\"l2\"))\\nX_test3, Y_test3 = blabla(training_df[training_df[\\'Sentiment\\']>2], 3, 0, label_col=\"Sentiment\", model=SGDClassifier(loss=\"hinge\", penalty=\"l2\"))'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/diaa/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/home/diaa/anaconda2/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/diaa/anaconda2/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1183\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m             \u001b[0;32mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1186\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SGDClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_test1, Y_test1 = blabla(training_df, 1, 1, label_col='3class_sent', model=SGDClassifier(loss=\"hinge\", penalty=\"l2\"))\n",
    "X_test2, Y_test2 = blabla(training_df[training_df['Sentiment']<2], 2, 0, label_col=\"Sentiment\", model=SGDClassifier(loss=\"hinge\", penalty=\"l2\"))\n",
    "X_test3, Y_test3 = blabla(training_df[training_df['Sentiment']>2], 3, 0, label_col=\"Sentiment\", model=SGDClassifier(loss=\"hinge\", penalty=\"l2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v1 = pickle.load(open(\"%s/vectorizer_1.pickle\" %PICKLED_DIR, \"rb\"))\n",
    "model1 = pickle.load(open(\"%s/model_1.pickle\" %PICKLED_DIR, \"rb\"))\n",
    "\n",
    "v2 = pickle.load(open(\"%s/vectorizer_2.pickle\" %PICKLED_DIR, \"rb\"))\n",
    "model2 = pickle.load(open(\"%s/model_2.pickle\" %PICKLED_DIR, \"rb\"))\n",
    "\n",
    "v3 = pickle.load(open(\"%s/vectorizer_3.pickle\" %PICKLED_DIR, \"rb\"))\n",
    "model3 = pickle.load(open(\"%s/model_3.pickle\" %PICKLED_DIR, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 25551, 1: 11518, -1: 9749})\n",
      "Counter({0: 23830, 1: 12614, -1: 10374})\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.72      0.68      0.70     10374\n",
      "          0       0.75      0.81      0.78     23830\n",
      "          1       0.78      0.71      0.74     12614\n",
      "\n",
      "avg / total       0.75      0.75      0.75     46818\n",
      "\n",
      "accuracy :  0.751206800803\n"
     ]
    }
   ],
   "source": [
    "# v1 = pickle.load(open(\"vectorizer_1.pickle\", \"rb\"))\n",
    "# m1 = pickle.load(open(\"model_1.pickle\", \"rb\"))\n",
    "\n",
    "too1 = v1.transform(X_test1)\n",
    "r1 = model1.predict(too1)\n",
    "\n",
    "print Counter(r1.tolist())\n",
    "print Counter(Y_test1[:, 1])\n",
    "\n",
    "print_report(Y_test1[:, 1], r1, ['-1', '0', '1'])\n",
    "#              precision    recall  f1-score   support\n",
    "\n",
    "#          -1       0.72      0.67      0.69     10413\n",
    "#           0       0.74      0.81      0.77     23780\n",
    "#           1       0.78      0.70      0.74     12625\n",
    "\n",
    "# avg / total       0.75      0.75      0.75     46818\n",
    "\n",
    "# accuracy :  0.747853389722"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 7630, 0: 2674})\n",
      "Counter({1: 8169, 0: 2135})\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.46      0.58      0.51      2135\n",
      "          1       0.88      0.82      0.85      8169\n",
      "\n",
      "avg / total       0.79      0.77      0.78     10304\n",
      "\n",
      "accuracy :  0.772030279503\n"
     ]
    }
   ],
   "source": [
    "# v2 = pickle.load(open(\"vectorizer_2.pickle\", \"rb\"))\n",
    "# m2 = pickle.load(open(\"model_2.pickle\", \"rb\"))\n",
    "\n",
    "too2 = v2.transform(X_test2)\n",
    "r2 = model2.predict(too2)\n",
    "\n",
    "print Counter(r2.tolist())\n",
    "print Counter(Y_test2[:, 0])\n",
    "\n",
    "\n",
    "print_report(Y_test2[:, 0], r2, ['0', '1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({3: 9258, 4: 3382})\n",
      "Counter({3: 9865, 4: 2775})\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          3       0.82      0.88      0.85      9258\n",
      "          4       0.59      0.48      0.53      3382\n",
      "\n",
      "avg / total       0.76      0.77      0.76     12640\n",
      "\n",
      "accuracy :  0.769857594937\n"
     ]
    }
   ],
   "source": [
    "# v3 = pickle.load(open(\"vectorizer_3.pickle\", \"rb\"))\n",
    "# m3 = pickle.load(open(\"model_3.pickle\", \"rb\"))\n",
    "\n",
    "too3 = v3.transform(X_test3)\n",
    "r3 = model3.predict(too3)\n",
    "\n",
    "print Counter(r3.tolist())\n",
    "print Counter(Y_test3[:,0])\n",
    "\n",
    "print_report(r3, Y_test3[:, 0], ['3', '4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global_predicted_res = []\n",
    "# print Counter(test1[-1][:, 1])\n",
    "# for i, t in enumerate(test1[0]):\n",
    "#     if test1[-1][i, 1] == -1: #NEGATIVE\n",
    "#         global_predicted_res.append(m2.predict(v2.transform([t]))[0])\n",
    "#     elif test1[-1][i, 1] == 1:\n",
    "#         global_predicted_res.append(m3.predict(v3.transform([t]))[0])\n",
    "#     else:\n",
    "#         global_predicted_res.append(2)\n",
    "# # print global_predicted_res[:5]\n",
    "# #     print Counter(global_predicted_res)\n",
    "# print_report(global_predicted_res, test1[-1][:, 0], ['0', '1', '2', '3', '4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({2: 23830, 3: 9839, 1: 8279, 4: 2775, 0: 2095})\n"
     ]
    }
   ],
   "source": [
    "def predict_pipeline(data, model1, v1, model2, v2, model3, v3):\n",
    "    \n",
    "#     predicted_res = []\n",
    "    \n",
    "    temp1 = v1.transform(data)\n",
    "    pred = model1.predict(temp1)\n",
    "    \n",
    "    for i, t in enumerate(data):\n",
    "        if pred[i] == -1: #NEGATIVE\n",
    "            pred[i] = (model2.predict(v2.transform([t]))[0])\n",
    "        elif pred[i] == 1: #POSITIVE\n",
    "#             pred[i] = 15\n",
    "            pred[i] = (model3.predict(v3.transform([t]))[0])\n",
    "        else:\n",
    "            pred[i] =(2)\n",
    "    \n",
    "    return pred\n",
    "print Counter(Y_test1[:, 0])\n",
    "ress = predict_pipeline(X_test1, model1, v1, model2, v2, model3, v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.54      0.65      3103\n",
      "          1       0.49      0.61      0.55      6646\n",
      "          2       0.81      0.75      0.78     25551\n",
      "          3       0.52      0.65      0.58      7775\n",
      "          4       0.81      0.60      0.69      3743\n",
      "\n",
      "avg / total       0.71      0.69      0.70     46818\n",
      "\n",
      "accuracy :  0.689307531291\n"
     ]
    }
   ],
   "source": [
    "print_report(ress, Y_test1[:, 0], ['0', '1', '2', '3', '4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'PhraseId', u'SentenceId', u'Phrase'], dtype='object')\n",
      "66292\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"%s/test.tsv\" %DATA_DIR, error_bad_lines=False , sep='\\t')\n",
    "print test_df.keys()\n",
    "print len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_res = predict_pipeline(test_df['Phrase'].tolist(), model1, v1, model2, v2, model3, v3)\n",
    "\n",
    "# tdf = pd.DataFrame({'PhraseId': test_df['PhraseId'].tolist(), 'Sentiment': test_res})\n",
    "# tdf.to_csv(\"3classes_sent.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# submit(model, countVec, \"LinearSVM_CVec_without_downsampling_balanced_p\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
